------------------
NTK notes
------------------

------------------
what is kernel
------------------
	kernel method is a method to map data input space into 
	a higher dimensional Hilbert space, such that 
	K(x1, x2) = <phi(x1), phi(x2)> 
	
	you need the higher-dim to make input data linearly separable
	where phi is the transformation and <.> is the inner product
	We do this because we can compute the dot product in the high 
	dim feature space using kernels without knowing how to compute phi()

------------------
What is NTK
------------------
	NN: y = f(w, x)
	Lazy training: where learnable weights barely change over training steps
	First order Taylor approximation around the weight initialization w_0
	f(w, x) ~= f(w_0, x) + (grad_w)f(w_0, x)(w - w_0) + ...
	let phi(x) = (grad_w)f(w_0, x)
	this behaves as a linear model in w
	the kernel function 
	ker(x, x') = <phi(x), phi(x')> is called NTK

	ultra-wide NN can be simplified as a linear kernel regression, and this kernel is called NTK

------------------
Spectural Bias
------------------
	neural networks are biased towards learning less complex functions
	The priority of learning functions with low complexity might be at the core of 
	explaining generalization ability of neural network

	The concept of spectral bias is appealing because this may intuitively explain
	why over-parameterized neural networks can achieve a good generalization 
	performance without overfitting

------------------
Hilbert Space
------------------

	vector(linear) space:	closure under scaler product and addtion
	
	metric d:
		- d(x, y) >= 0 		nonnegativity
		- d(x, y) = d(y, x)	symmetry
		- d(x, y) <= d(x, z) + d(z, y) for all x, y, z in X, triangle inequality

	norm: (intuition: distance from 0)
		- ||x|| = 0
		- ||ax|| = |a| ||x||
		- ||x + y|| <= ||x|| + ||y||
	
	inner product (x, y):
		- symmetric: (x, y) = (y, x)
		- positive definite: (x, x) > 0
		- bi-linear: (ax + by, z) = a(x, z) + b(y, z)	

	a metric space is a set X equipped with a metric d.

	normed vector space:	defines norms and distance -> metric
	A normed linear space is a metric space with the metric d(x, y) = ||x - y||

	inner product space:	defines angles in vector space

	Cauchy sequence: 
	complete space (Cauchy space): every Cauchy sequence in M converges in M (has a limit that is also in M) 		
	Banach space:		a complete normed vector space

	Hilbert space is a complete vector space equipped with an inner product operation

	Summary
	 - metric + linear => linear metric space
	 - norm + linear => normed linear space + completeness => Banach space
	 - inner product + linear = inner product space + completeness => Hilbert space

------------------
RKHS
------------------
	- function of one variable => inf-dim vector
	- function of two variables => inf-dim matrix

	- kernel function:
		- positive definite
		- symmetric
	- kernel function can be seen as infinite-dim matrix
		- it is a symmetric real matrix
		- eigenvalue decomposition

	- in RKHS, inner product is constructed(reproduced) by kernel functions
 	- reproducing property:
		- K(x, y) = (K(x,·), K(y,·))

 	- can also be defined with dirac evaluation functional


------------------
Laplacian Kernel
------------------
	- K(x, y) = exp(-||x - y|| / sigma)
	- similar to Gaussian kernel - without square

------------------
eigenvalue decay
------------------
	- write a kernel k(x, x') as a matrix 
	- do spectral decomposition (eigenvalue decomposition) on this matrix
	- the eigenvalues are decreasing on the diagnal and the rate of that decrease
	  is called eigenvalue decay

------------------
gradient flow
------------------
	- gradient decent with infinitesimally small learning rate

------------------
Project
------------------
	understand the spectral bias of deep learning through the study of NTK.

Goal #1: 
	Quantify the learnability of the kernel method based on NTK. 
        This is expected to be done by investigating the reproducing 
	kernel Hilbert space (RKHS) associated to the NTK.

Experiment ideas:
 - 	What happens to the RHKS of the NTK if ReLU is replaced by other 
   	activation function, such as {ReLU}^k or smooth activation function 
   	such as sin, cos?
	
 - 	compute NTK and the Laplace kernel for a ultra-wide one hidden layer NN
	and observe their eigenvalue decay

 - 	Multiple activation functions can sometimes achieve much better 
   	approximation power than the ones with a single activation. 
   	Can we show that the spectral bias of NTK can be alleviated if one 
   	uses multiple activation functions?


paper 1, 3, 4:
   	the RKHS of the NTK for a deep ReLU neural network is identical to 
   	the RKHS of a Laplace kernel.

paper 2, 7: 
	spectral bias

paper 5: 
	NTK orginal paper

Goal #2: 
	Reducing the spectral bias of NTK by reparameterization of the input. 
	Consider the regression problem of fitting a target function containing 
	multiple frequencies using neural networks. 
	
	Due to the spectral bias, the high frequency mode is much difficult to learn 
	compared to the lower modes. 

	However, if we first reparameterize the input via certain nonlinear transform, 
	such as sin-transform so that the input is encoded to certain frequency mode, 
	then could we show that the spectrum of the NTK after reparameterization 
	decays slower than the original NTK?

paper 6, 8, 9





------------------
ReLU network
------------------

with a ReLU network, NTK and Laplace kernel has the same eigenvalue decay

Task:

 - calculate a kernel K(x_i, x_j)
	- 2-layer NN (1 hidden layer)
	- Take 1st order derivative of the weight 
	- If you go back to the part where we first Taylor expanded the network function, 
	  we saw that the linearized model has a feature map ϕ(x)=∇wf(x,w0).
	- The kernel matrix corresponding to this feature map 
	  is obtained by taking pairwise inner products between 
	  the feature maps of all the data points. This is exactly H(w0)

 - randomly sample (x_i, x_j) from a uniform distribution [0, 1] 
	- sample 100 x_i, and 100 x_j
 - write this as a kernel matrix (100, 100)
 - continued kernel is an operator (like an integral) - we approximate it with a matrix
 - then observe the spectrum (by eigenvalue decomposition) - observe the eigen decay

 - compare the kernel of NTK and Laplace kernel
 - under a different activation function e.g. {ReLU}^k
 - tune the value of alpha in ||x - y||^a and see which one is the closest to NTK eigenvalue decay









